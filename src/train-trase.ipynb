{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1aQDBPt2Yg4hhNVFfLycQslLlfa3H5naL","authorship_tag":"ABX9TyNdza7h9wn6KHN6ORPpJZLa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wAjFzudCeVK0","executionInfo":{"status":"ok","timestamp":1748488555708,"user_tz":420,"elapsed":19220,"user":{"displayName":"Jayson Meribe","userId":"18347816154755748104"}},"outputId":"82666eaa-3267-4a21-fb61-b6e53fca4128"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://download.pytorch.org/whl/cu118\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n","Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n","Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: x-transformers in /usr/local/lib/python3.11/dist-packages (2.3.9)\n","Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from x-transformers) (0.8.1)\n","Requirement already satisfied: einx>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from x-transformers) (0.3.0)\n","Requirement already satisfied: loguru in /usr/local/lib/python3.11/dist-packages (from x-transformers) (0.7.3)\n","Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.11/dist-packages (from x-transformers) (24.2)\n","Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from x-transformers) (2.6.0+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->x-transformers) (2.0.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->x-transformers) (1.13.1)\n","Requirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->x-transformers) (2.4.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (11.8.87)\n","Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (11.11.3.6)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (10.3.0.86)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (11.4.1.48)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (11.7.5.86)\n","Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (11.8.86)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->x-transformers) (3.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->einx>=0.3.0->x-transformers) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->x-transformers) (3.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.29.1)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n","Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjmeribe\u001b[0m (\u001b[33mstanford-curis-jmeribe\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250529_031554-fnex50m6</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/stanford-curis-jmeribe/generating-imu-data-two/runs/fnex50m6' target=\"_blank\">whole-flower-57</a></strong> to <a href='https://wandb.ai/stanford-curis-jmeribe/generating-imu-data-two' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/stanford-curis-jmeribe/generating-imu-data-two' target=\"_blank\">https://wandb.ai/stanford-curis-jmeribe/generating-imu-data-two</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/stanford-curis-jmeribe/generating-imu-data-two/runs/fnex50m6' target=\"_blank\">https://wandb.ai/stanford-curis-jmeribe/generating-imu-data-two/runs/fnex50m6</a>"]},"metadata":{}}],"source":["!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","!pip install x-transformers\n","!pip install matplotlib\n","!pip install einops\n","!pip install wandb\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!cd /content/drive/MyDrive/realistic-imu/src\n","\n","import wandb\n","wandb.login()\n","\n","import csv\n","import matplotlib.pyplot as plt\n","import os\n","import sys\n","import wandb\n","import pickle\n","import re\n","\n","# Add the source directory to the system path\n","sys.path.append('/content/drive/MyDrive/realistic-imu/src')\n","\n","from trase_dataset import TraseDataset\n","from trase import Trase, TraseLoss\n","\n","\n","if 'ipykernel' in sys.modules:\n","    from tqdm.notebook import tqdm\n","else:\n","    from tqdm import tqdm\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.utils.data import DataLoader\n","\n","\n","torch.cuda.empty_cache()\n","\n","torch.set_float32_matmul_precision('high')\n","\n","device = torch.device(\"cuda\")\n","EPOCHS = 6000\n","LEARNING_RATE = 1e-5\n","WEIGHT_DECAY = 1e-4\n","data_path = \"/content/drive/MyDrive/realistic-imu/data/realistic-imu-dataset/\"\n","base_path = \"/content/drive/MyDrive/realistic-imu/data/realistic-imu-dataset/models\"\n","D_MODEL = 1024\n","INPUT_EMBEDDING_DIM = 408\n","NUM_ENCODERS = 3\n","FEED_FORWARD_DIM = 2048\n","DROPOUT = 0.1\n","HEADS = 8\n","TOTAL_VAR_WEIGHT = 1e-2\n","\n","\n","run = wandb.init(\n","    project=\"generating-imu-data-two\",\n","    config={\n","        \"learning_rate\": LEARNING_RATE,\n","        \"epochs\": EPOCHS,\n","        \"weight_decay\": WEIGHT_DECAY,\n","        \"d_model\": D_MODEL,\n","        \"input_embedding_dim\": INPUT_EMBEDDING_DIM,\n","        \"num_encoders\": NUM_ENCODERS,\n","        \"feed_forward_dim\": FEED_FORWARD_DIM,\n","        \"dropout\": DROPOUT,\n","        \"heads\": HEADS,\n","        \"total_var_weight\": TOTAL_VAR_WEIGHT\n","    }\n",")"]},{"cell_type":"code","source":["model = Trase(d_model=D_MODEL,\n","              inp_emb_dim=INPUT_EMBEDDING_DIM,\n","              device=device,\n","              num_encoders=NUM_ENCODERS,\n","              dim_feed_forward=FEED_FORWARD_DIM,\n","              dropout=DROPOUT,\n","              heads=HEADS).to(device)"],"metadata":{"id":"gYqxCYvnFqAF","executionInfo":{"status":"ok","timestamp":1748488556217,"user_tz":420,"elapsed":499,"user":{"displayName":"Jayson Meribe","userId":"18347816154755748104"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# model = torch.compile(model)\n","\n","optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","\n","\n","train_path = os.path.join(data_path, \"train.pkl\")\n","dev_path = os.path.join(data_path, \"dev.pkl\")\n","test_path = os.path.join(data_path, \"test.pkl\")\n","\n","train_dataset = TraseDataset(train_path)\n","dev_dataset = TraseDataset(dev_path)\n","# test_dataset = TraseDataset(test_path)\n","\n","identity_collate = lambda batch: batch\n","\n","\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=identity_collate)\n","dev_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False, collate_fn=identity_collate)\n","# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","criterion = TraseLoss(total_var_weight=TOTAL_VAR_WEIGHT)\n","\n","\n","model.to(device)"],"metadata":{"id":"qvAgwNAux5iH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748488558689,"user_tz":420,"elapsed":2466,"user":{"displayName":"Jayson Meribe","userId":"18347816154755748104"}},"outputId":"c6e31032-a8b6-4682-ea23-866a85c49c89"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Trase(\n","  (linear1): Linear(in_features=408, out_features=1024, bias=True)\n","  (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  (activation1): GELU(approximate='none')\n","  (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n","  (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  (activation2): GELU(approximate='none')\n","  (encoder): Encoder(\n","    (transformer_encoder): Encoder(\n","      (layers): ModuleList(\n","        (0): ModuleList(\n","          (0): ModuleList(\n","            (0): LayerNorm(\n","              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n","            )\n","            (1-2): 2 x None\n","          )\n","          (1): Attention(\n","            (to_q): Linear(in_features=1024, out_features=512, bias=False)\n","            (to_k): Linear(in_features=1024, out_features=512, bias=False)\n","            (to_v): Linear(in_features=1024, out_features=512, bias=False)\n","            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n","            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n","            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n","            (merge_heads): Rearrange('b h n d -> b n (h d)')\n","            (attend): Attend(\n","              (attn_dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (to_out): Linear(in_features=512, out_features=1024, bias=False)\n","            (sublayer_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): Residual()\n","        )\n","        (1): ModuleList(\n","          (0): ModuleList(\n","            (0): LayerNorm(\n","              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n","            )\n","            (1-2): 2 x None\n","          )\n","          (1): FeedForward(\n","            (ff): Sequential(\n","              (0): Sequential(\n","                (0): Linear(in_features=1024, out_features=4096, bias=True)\n","                (1): GELU(approximate='none')\n","              )\n","              (1): Dropout(p=0.1, inplace=False)\n","              (2): Linear(in_features=4096, out_features=1024, bias=True)\n","              (3): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): Residual()\n","        )\n","        (2): ModuleList(\n","          (0): ModuleList(\n","            (0): LayerNorm(\n","              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n","            )\n","            (1-2): 2 x None\n","          )\n","          (1): Attention(\n","            (to_q): Linear(in_features=1024, out_features=512, bias=False)\n","            (to_k): Linear(in_features=1024, out_features=512, bias=False)\n","            (to_v): Linear(in_features=1024, out_features=512, bias=False)\n","            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n","            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n","            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n","            (merge_heads): Rearrange('b h n d -> b n (h d)')\n","            (attend): Attend(\n","              (attn_dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (to_out): Linear(in_features=512, out_features=1024, bias=False)\n","            (sublayer_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): Residual()\n","        )\n","        (3): ModuleList(\n","          (0): ModuleList(\n","            (0): LayerNorm(\n","              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n","            )\n","            (1-2): 2 x None\n","          )\n","          (1): FeedForward(\n","            (ff): Sequential(\n","              (0): Sequential(\n","                (0): Linear(in_features=1024, out_features=4096, bias=True)\n","                (1): GELU(approximate='none')\n","              )\n","              (1): Dropout(p=0.1, inplace=False)\n","              (2): Linear(in_features=4096, out_features=1024, bias=True)\n","              (3): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): Residual()\n","        )\n","        (4): ModuleList(\n","          (0): ModuleList(\n","            (0): LayerNorm(\n","              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n","            )\n","            (1-2): 2 x None\n","          )\n","          (1): Attention(\n","            (to_q): Linear(in_features=1024, out_features=512, bias=False)\n","            (to_k): Linear(in_features=1024, out_features=512, bias=False)\n","            (to_v): Linear(in_features=1024, out_features=512, bias=False)\n","            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n","            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n","            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n","            (merge_heads): Rearrange('b h n d -> b n (h d)')\n","            (attend): Attend(\n","              (attn_dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (to_out): Linear(in_features=512, out_features=1024, bias=False)\n","            (sublayer_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): Residual()\n","        )\n","        (5): ModuleList(\n","          (0): ModuleList(\n","            (0): LayerNorm(\n","              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n","            )\n","            (1-2): 2 x None\n","          )\n","          (1): FeedForward(\n","            (ff): Sequential(\n","              (0): Sequential(\n","                (0): Linear(in_features=1024, out_features=4096, bias=True)\n","                (1): GELU(approximate='none')\n","              )\n","              (1): Dropout(p=0.1, inplace=False)\n","              (2): Linear(in_features=4096, out_features=1024, bias=True)\n","              (3): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): Residual()\n","        )\n","      )\n","      (layer_integrators): ModuleList(\n","        (0-5): 6 x None\n","      )\n","      (rotary_pos_emb): RotaryEmbedding()\n","      (adaptive_mlp): Identity()\n","      (final_norm): LayerNorm(\n","        (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n","      )\n","      (skip_combines): ModuleList(\n","        (0-5): 6 x None\n","      )\n","    )\n","    (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (feed_forward): Sequential(\n","      (0): Linear(in_features=1024, out_features=4096, bias=True)\n","      (1): GELU(approximate='none')\n","      (2): Linear(in_features=4096, out_features=1024, bias=True)\n","    )\n","  )\n","  (linear3): Linear(in_features=1024, out_features=1024, bias=True)\n","  (layer_norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  (activation3): GELU(approximate='none')\n","  (noise_regressor): Noise_Regressor(\n","    (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (hidden_state_to_noise_params): Linear(in_features=1024, out_features=864, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["class CheckpointSaver:\n","    def __init__(self, model, initial_best_loss=float(\"inf\")):\n","        self.best_dev_loss = initial_best_loss\n","        self.model = model\n","\n","    def save_checkpoint(self, dev_loss: float):\n","        # dev_loss is now guaranteed to be a float\n","        if dev_loss <= self.best_dev_loss:\n","            os.makedirs(f\"weights/models-{wandb.run.name}\", exist_ok=True)\n","            torch.save(self.model.state_dict(),\n","                       f\"weights/models-{wandb.run.name}/best.pt\")\n","            wandb.save(f\"weights/models-{wandb.run.name}/best.pt\")\n","            self.best_dev_loss = dev_loss\n","            wandb.log({\"best_dev_loss\": dev_loss})\n","\n","        wandb.log({\"dev_loss\": dev_loss})\n","\n","saver = CheckpointSaver(model)"],"metadata":{"id":"86VUowKRyGUf","executionInfo":{"status":"ok","timestamp":1748488558698,"user_tz":420,"elapsed":4,"user":{"displayName":"Jayson Meribe","userId":"18347816154755748104"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def train_model():\n","    curr_loss = 0\n","    model.train()\n","\n","\n","    for data in train_loader:\n","\n","      data = data[0]\n","      mocap_data = data[\"inputs\"]\n","      real_acc = data[\"accelerations_output\"]\n","      real_angular_vel = data[\"angular_velocities_output\"] if data[\"angular_velocities_output\"] is not None else None\n","      mask = data[\"output_mask\"].T\n","      weights = data[\"weights\"].T.repeat_interleave(3, dim=0)\n","\n","\n","      optimizer.zero_grad()\n","\n","      kinematics, acc_output, acc_std, gyro_output, gyro_std = model(mocap_data)\n","\n","      loss = criterion(kinematics=kinematics * mask * weights,\n","                       acc_mean = acc_output * mask * weights,\n","                       acc_std = acc_std * weights,\n","                       real_acc = real_acc * mask * weights,\n","                       gyro_mean = gyro_output,\n","                       gyro_std = gyro_std,\n","                       real_gyro = real_angular_vel,\n","                       include_gyro = real_angular_vel is not None)\n","\n","      loss.backward()\n","      optimizer.step()\n","      scheduler.step()\n","\n","      curr_loss += loss.item()\n","\n","    return curr_loss / len(train_loader)\n","\n","\n","\n","def evaluate_model(data_loader):\n","    curr_loss = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","      for data in data_loader:\n","        data = data[0]\n","        mocap_data = data[\"inputs\"]\n","        real_acc = data[\"accelerations_output\"]\n","        real_angular_vel = data[\"angular_velocities_output\"] if data[\"angular_velocities_output\"] is not None else None\n","        mask = data[\"output_mask\"].T\n","        weights = data[\"weights\"].T.repeat_interleave(3, dim=0)\n","\n","\n","        kinematics, acc_output, acc_std, gyro_output, gyro_std = model(mocap_data)\n","\n","        loss = criterion(kinematics=kinematics * mask * weights,\n","                        acc_mean = acc_output * mask * weights,\n","                        acc_std = acc_std * weights,\n","                        real_acc = real_acc * mask * weights,\n","                        gyro_mean = gyro_output,\n","                        gyro_std = gyro_std,\n","                        real_gyro = real_angular_vel,\n","                        include_gyro = real_angular_vel is not None)\n","\n","\n","        curr_loss += loss.item()\n","\n","    return curr_loss / len(data_loader)"],"metadata":{"id":"ylEbs7yOyQJR","executionInfo":{"status":"ok","timestamp":1748488558707,"user_tz":420,"elapsed":3,"user":{"displayName":"Jayson Meribe","userId":"18347816154755748104"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["wandb.log({\"train_loss\": evaluate_model(train_loader)})\n","\n","dev_loss_value = evaluate_model(dev_loader)\n","wandb.log({\"dev_loss\": dev_loss_value})\n","saver.save_checkpoint(dev_loss_value)\n","\n","progress_bar = tqdm(range(EPOCHS), desc=\"Training Progress\", position=0, leave=True)\n","\n","for epoch in progress_bar:\n","\n","    train_loss = train_model()\n","    wandb.log({\"train_loss\": train_loss})\n","\n","    if (epoch + 1) % 1 == 0:\n","        loss_value = evaluate_model(dev_loader)\n","        saver.save_checkpoint(loss_value)\n","    else:\n","        test_loss = None\n","\n","\n","    # Log the current learning rate\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    progress_desc = f\"Epoch {epoch + 1}/{EPOCHS} | Train Loss: {train_loss:.4f} | LR: {current_lr:.12f}\"\n","    if test_loss is not None:\n","        progress_desc += f\" | Test Loss: {test_loss:.4f}\"\n","    progress_bar.set_description(progress_desc)"],"metadata":{"id":"FtRCDz1ByU_t","colab":{"base_uri":"https://localhost:8080/","height":420},"executionInfo":{"status":"error","timestamp":1748488558972,"user_tz":420,"elapsed":261,"user":{"displayName":"Jayson Meribe","userId":"18347816154755748104"}},"outputId":"b1bb5ab9-2112-4482-cccc-803bc5b28f8a"},"execution_count":6,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 7.47 GiB. GPU 0 has a total capacity of 22.16 GiB of which 6.42 GiB is free. Process 86206 has 15.73 GiB memory in use. Of the allocated memory 15.50 GiB is allocated by PyTorch, and 17.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-7eed7f8b57bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdev_loss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"dev_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdev_loss_value\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_loss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-5333bb4a4661>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mkinematics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgyro_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgyro_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmocap_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         loss = criterion(kinematics=kinematics * mask * weights,\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/realistic-imu/src/trase.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mencoded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mresidual_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/realistic-imu/src/trase.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# 1) layer‑norm → x‑transformers encoder (self‑attention + FFN)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx_norm\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mx_encoded\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# 2) post‑norm + our custom FF block + residual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/x_transformers/x_transformers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context, mask, context_mask, attn_mask, self_attn_kv_mask, mems, mem_masks, seq_start_pos, cache, cache_age, return_hiddens, rotary_pos_emb, pos, context_pos, attn_bias, deep_embeds_and_ids, condition, in_attn_cond, layers_execute_order)\u001b[0m\n\u001b[1;32m   2598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2600\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attn_kv_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotary_pos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotary_pos_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_attn_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_mem_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_residual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_self_attn_value_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_intermediates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2601\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlayer_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_cross_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_attn_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_residual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_cross_attn_value_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcross_attn_rotary_pos_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_intermediates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/x_transformers/x_transformers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context, mask, context_mask, attn_mask, rel_pos, attn_bias, rotary_pos_emb, context_rotary_pos_emb, pos, prev_attn, mem, mem_mask, return_intermediates, cache, value_residual)\u001b[0m\n\u001b[1;32m   1818\u001b[0m         \u001b[0;31m# attention is all we need\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1820\u001b[0;31m         out, intermediates = self.attend(\n\u001b[0m\u001b[1;32m   1821\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_attn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/x_transformers/attend.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, mask, attn_bias, prev_attn)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mpre_softmax_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2140\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2142\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.47 GiB. GPU 0 has a total capacity of 22.16 GiB of which 6.42 GiB is free. Process 86206 has 15.73 GiB memory in use. Of the allocated memory 15.50 GiB is allocated by PyTorch, and 17.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}]},{"cell_type":"code","source":["os.makedirs(base_path, exist_ok=True)\n","model_files = [f for f in os.listdir(base_path) if re.match(r\"model_\\d+\\.pkl\", f)]\n","\n","if model is not None:\n","    if model_files:\n","        max_num = max(int(re.search(r\"model_(\\d+)\\.pkl\", f).group(1)) for f in model_files)\n","    else:\n","        max_num = 0\n","    new_model_name = f\"model_{max_num + 1}.pkl\"\n","    save_path = os.path.join(base_path, new_model_name)\n","    with open(save_path, 'wb') as file:\n","        pickle.dump(model, file)\n","    print(f\"Model saved to: {save_path}\")\n","else:\n","    if not model_files:\n","        raise FileNotFoundError(\"No model files found in the directory.\")\n","    latest_model_file = max(model_files, key=lambda f: int(re.search(r\"model_(\\d+)\\.pkl\", f).group(1)))\n","    load_path = os.path.join(base_path, latest_model_file)\n","    with open(load_path, 'rb') as file:\n","        model = pickle.load(file)\n","    print(f\"Loaded model from: {load_path}\")"],"metadata":{"id":"zgkbUb7cyZht","executionInfo":{"status":"aborted","timestamp":1748488559061,"user_tz":420,"elapsed":141,"user":{"displayName":"Jayson Meribe","userId":"18347816154755748104"}}},"execution_count":null,"outputs":[]}]}