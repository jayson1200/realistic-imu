{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1aQDBPt2Yg4hhNVFfLycQslLlfa3H5naL","authorship_tag":"ABX9TyN2mliiwg8MMP/YTb9C51FP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wAjFzudCeVK0"},"outputs":[],"source":["!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","!pip install x-transformers\n","!pip install matplotlib\n","!pip install einops\n","!pip install wandb\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!cd /content/drive/MyDrive/realistic-imu/src\n","\n","import wandb\n","wandb.login()\n","\n","import csv\n","import matplotlib.pyplot as plt\n","import os\n","import sys\n","import wandb\n","import pickle\n","import re\n","\n","# Add the source directory to the system path\n","sys.path.append('/content/drive/MyDrive/realistic-imu/src')\n","\n","from trase_dataset import TraseDataset\n","from trase import Trase, TraseLoss\n","\n","\n","if 'ipykernel' in sys.modules:\n","    from tqdm.notebook import tqdm\n","else:\n","    from tqdm import tqdm\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.amp import autocast, GradScaler\n","from torch.utils.data import DataLoader\n","from torch.profiler import profile, record_function, ProfilerActivity\n","\n","\n","torch.cuda.empty_cache()\n","\n","torch.set_float32_matmul_precision('high')\n","\n","device = torch.device(\"cuda\")\n","EPOCHS = 500\n","LEARNING_RATE = 1e-5\n","WEIGHT_DECAY = 1e-3\n","data_path = \"/content/drive/MyDrive/realistic-imu/data/realistic-imu-dataset/\"\n","base_path = \"/content/drive/MyDrive/realistic-imu/data/realistic-imu-dataset/models\"\n","D_MODEL = 1024\n","INPUT_EMBEDDING_DIM = 408\n","NUM_ENCODERS = 1\n","FEED_FORWARD_DIM = 2048\n","DROPOUT = 0.1\n","HEADS = 8\n","TOTAL_VAR_WEIGHT = 1e-2\n","\n","\n","run = wandb.init(\n","    project=\"generating-imu-data-two\",\n","    config={\n","        \"learning_rate\": LEARNING_RATE,\n","        \"epochs\": EPOCHS,\n","        \"weight_decay\": WEIGHT_DECAY,\n","        \"d_model\": D_MODEL,\n","        \"input_embedding_dim\": INPUT_EMBEDDING_DIM,\n","        \"num_encoders\": NUM_ENCODERS,\n","        \"feed_forward_dim\": FEED_FORWARD_DIM,\n","        \"dropout\": DROPOUT,\n","        \"heads\": HEADS,\n","        \"total_var_weight\": TOTAL_VAR_WEIGHT\n","    }\n",")"]},{"cell_type":"code","source":["model = Trase(d_model=D_MODEL,\n","              inp_emb_dim=INPUT_EMBEDDING_DIM,\n","              device=device,\n","              num_encoders=NUM_ENCODERS,\n","              dim_feed_forward=FEED_FORWARD_DIM,\n","              dropout=DROPOUT,\n","              heads=HEADS).to(device)"],"metadata":{"id":"gYqxCYvnFqAF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = torch.compile(model)\n","\n","\n","train_path = os.path.join(data_path, \"train.pkl\")\n","dev_path = os.path.join(data_path, \"dev.pkl\")\n","test_path = os.path.join(data_path, \"test.pkl\")\n","\n","train_dataset = TraseDataset(train_path)\n","dev_dataset = TraseDataset(dev_path)\n","# test_dataset = TraseDataset(test_path)\n","\n","identity_collate = lambda batch: batch\n","\n","\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=identity_collate)\n","dev_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False, collate_fn=identity_collate)\n","# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","criterion = TraseLoss(total_var_weight=TOTAL_VAR_WEIGHT)\n","\n","\n","model.to(device)"],"metadata":{"id":"qvAgwNAux5iH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CheckpointSaver:\n","    def __init__(self, model, initial_best_loss=float(\"inf\")):\n","        self.best_dev_loss = initial_best_loss\n","        self.model = model\n","\n","    def save_checkpoint(self, dev_loss: float):\n","        # dev_loss is now guaranteed to be a float\n","        if dev_loss <= self.best_dev_loss:\n","            os.makedirs(f\"weights/models-{wandb.run.name}\", exist_ok=True)\n","            torch.save(self.model.state_dict(),\n","                       f\"weights/models-{wandb.run.name}/best.pt\")\n","            wandb.save(f\"weights/models-{wandb.run.name}/best.pt\")\n","            self.best_dev_loss = dev_loss\n","            wandb.log({\"best_dev_loss\": dev_loss})\n","\n","        wandb.log({\"dev_loss\": dev_loss})\n","\n","saver = CheckpointSaver(model)"],"metadata":{"id":"86VUowKRyGUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","\n","# 1) create a GradScaler for mixed precision\n","scaler = GradScaler()\n","\n","def train_model():\n","    curr_loss = 0.0\n","    model.train()\n","\n","    for data in train_loader:\n","        data = data[0]\n","        mocap_data = data[\"inputs\"].to(device)\n","        real_acc = data[\"accelerations_output\"].to(device)\n","        real_angular_vel = (data[\"angular_velocities_output\"].to(device)\n","                            if data[\"angular_velocities_output\"] is not None else None)\n","        mask = data[\"output_mask\"].T.to(device)\n","        weights = data[\"weights\"].T.repeat_interleave(3, dim=0).to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # 2) forward + loss inside autocast for mixed precision\n","        with autocast(\"cuda\"):\n","            kinematics, acc_output, acc_std, gyro_output, gyro_std = model(mocap_data)\n","            loss = criterion(\n","                kinematics=kinematics * mask * weights,\n","                acc_mean=acc_output * mask * weights,\n","                acc_std=acc_std * weights,\n","                real_acc=real_acc * mask * weights,\n","                gyro_mean=gyro_output,\n","                gyro_std=gyro_std,\n","                real_gyro=real_angular_vel,\n","                include_gyro=(real_angular_vel is not None)\n","            )\n","\n","        # 3) scale, backward, step\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        scheduler.step()\n","\n","        curr_loss += loss.item()\n","\n","    return curr_loss / len(train_loader)\n","\n","\n","\n","def evaluate_model(data_loader):\n","    curr_loss = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","      for data in data_loader:\n","        data = data[0]\n","        mocap_data = data[\"inputs\"]\n","        real_acc = data[\"accelerations_output\"]\n","        real_angular_vel = data[\"angular_velocities_output\"] if data[\"angular_velocities_output\"] is not None else None\n","        mask = data[\"output_mask\"].T\n","        weights = data[\"weights\"].T.repeat_interleave(3, dim=0)\n","\n","        kinematics, acc_output, acc_std, gyro_output, gyro_std = model(mocap_data)\n","\n","        loss = criterion(kinematics=kinematics * mask * weights,\n","                        acc_mean = acc_output * mask * weights,\n","                        acc_std = acc_std * weights,\n","                        real_acc = real_acc * mask * weights,\n","                        gyro_mean = gyro_output,\n","                        gyro_std = gyro_std,\n","                        real_gyro = real_angular_vel,\n","                        include_gyro = real_angular_vel is not None)\n","\n","\n","        curr_loss += loss.item()\n","\n","    return curr_loss / len(data_loader)"],"metadata":{"id":"ylEbs7yOyQJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dev_loss_value = evaluate_model(dev_loader)\n","\n","wandb.log({\"dev_loss\": dev_loss_value})\n","saver.save_checkpoint(dev_loss_value)\n","\n","wandb.log({\"train_loss\": evaluate_model(train_loader)})\n","\n","progress_bar = tqdm(range(EPOCHS), desc=\"Training Progress\", position=0, leave=True)\n","\n","for epoch in progress_bar:\n","    train_loss = train_model()\n","    wandb.log({\"train_loss\": train_loss})\n","\n","    if (epoch + 1) % 1 == 0:\n","        dev_loss_value = evaluate_model(dev_loader)\n","        saver.save_checkpoint(dev_loss_value)\n","    else:\n","        dev_loss_value = None\n","\n","\n","    # Log the current learning rate\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    progress_desc = f\"Epoch {epoch + 1}/{EPOCHS} | Train Loss: {train_loss:.4f} | LR: {current_lr:.12f}\"\n","    if dev_loss_value is not None:\n","        progress_desc += f\" | Dev loss: {dev_loss_value:.4f}\"\n","    progress_bar.set_description(progress_desc)"],"metadata":{"id":"FtRCDz1ByU_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.makedirs(base_path, exist_ok=True)\n","model_files = [f for f in os.listdir(base_path) if re.match(r\"model_\\d+\\.pkl\", f)]\n","\n","if model is not None:\n","    if model_files:\n","        max_num = max(int(re.search(r\"model_(\\d+)\\.pkl\", f).group(1)) for f in model_files)\n","    else:\n","        max_num = 0\n","    new_model_name = f\"model_{max_num + 1}.pkl\"\n","    save_path = os.path.join(base_path, new_model_name)\n","    with open(save_path, 'wb') as file:\n","        pickle.dump(model, file)\n","    print(f\"Model saved to: {save_path}\")\n","else:\n","    if not model_files:\n","        raise FileNotFoundError(\"No model files found in the directory.\")\n","    latest_model_file = max(model_files, key=lambda f: int(re.search(r\"model_(\\d+)\\.pkl\", f).group(1)))\n","    load_path = os.path.join(base_path, latest_model_file)\n","    with open(load_path, 'rb') as file:\n","        model = pickle.load(file)\n","    print(f\"Loaded model from: {load_path}\")"],"metadata":{"id":"zgkbUb7cyZht"},"execution_count":null,"outputs":[]}]}